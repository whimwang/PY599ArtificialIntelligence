# -*- coding: utf-8 -*-
"""Copy of Session 4: Search mechanisms.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BGn7soOl2E46HMuq1dX6VHCTledpcdjM

# **Search Methods for AI Applications**

1.   List item
2.   List item




## PY599 (Fall 2018): Applied Artificial Intelligence
## NC State University
###Dr. Behnam Kia
### https://appliedai.wordpress.ncsu.edu/


**Disclaimer**: Please note that these codes are simplified version of the algorithms, and they may not give the best, or expected performance that you could possibly get from these algorithms. The aim of this notebook is to help you understand the basics and the essence of these algorithms, and experiment with them. These basic codes are not deployment-ready or free-of-errors for real-world applications. To learn more about these algorithms please refer to text books that specifically study these algorithms, or contact me. - Behnam Kia

# Brute-Force Search
"""

import numpy as np
from scipy import optimize
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter


#Here we define function f. f is the function that we like to find its global minimum. Here f is the
#famous Rastrigin Function that has many local minima.
  
def f(z):
    x,y=z
    h = (x**2 - 10 * np.cos(2 * 3.14 * x)) +(y**2 - 10 * np.cos(2 * 3.14 * y)) + 20
    return (h)
  
 
# Let's take a look at how this function looks like. 

fig = plt.figure()
ax = fig.gca(projection='3d')

X = np.arange(-5, 5, 0.002)
Y = np.arange(-5, 5, 0.002)
X, Y = np.meshgrid(X, Y)
Z = f((X,Y))

surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=True) 
  
  
  
# We use Python's Scipy module and its built-in optimizer method, brute. Actually
# implementing brute force search is not that hard; define a grid, evaluate the 
#function over the grid points, and choose the point that has the minimum value.
# Of course the grid points should be close enough to make sure that we don't 
# miss a global minimum point between them. The rule of thumb is that the resolution of
# grid should be higher than the curvature of function surface, otherwise we 
# are going to miss those fine curves that fall between our grid points.
  
rranges = (slice(-4, 4,0.01), slice(-4, 4,0.01))

resbrute = optimize.brute(f, rranges,full_output=True,
                          finish=optimize.fmin)
print("The global minimum point is located at",resbrute[0])  

print("The function value at this global point is",resbrute[1])  

# For full description of how to use SciPy's brute force optimization and the arguments it takes, please see its official
#documentation at: https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.brute.html

"""# Homework
Try this technique on another function! Edit function f, and put a new function there and see whether it finds the global minimum point or not. Don't be afraid of going 3-D. But notice that if you define a 3-D function, you have to add another dimension to rranges as well.
"""

import numpy as np

def f(z):
    x,y=z
    h = np.power(x,2)+np.power(y,2)
    return h
  
fig = plt.figure()
ax = fig.gca(projection='3d')

X = np.arange(-5, 5, 0.002)
Y = np.arange(-5, 5, 0.002)
X, Y = np.meshgrid(X, Y)
Z=f((X,Y))

surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased= True) 


rranges = (slice(-4, 4,0.01), slice(-4, 4,0.01))

resbrute = optimize.brute(f, rranges,full_output=True,
                          finish=optimize.fmin)



print("The global minimum point is located at",resbrute[0])  

print("The function value at this global point is",resbrute[1])

"""#Homework: Random Search
Implement a pure random search method, and find global minimum point of Rastrigin Function.

Problem 1: **Searched Mechanism**
"""

import numpy as np
import random

def f(z):
    x,y=z
    h = (x**2 - 10 * np.cos(2 * 3.14 * x)) +(y**2 - 10 * np.cos(2 * 3.14 * y)) + 20
    return (h)
  

def optimization(initial,radius,iterations,func):
    if len([_ for _ in radius if _ < 1 == True])> 0:
         raise ValueError("Negative Radius !")
    if not len(initial)==len(radius):
         raise ValueError("Different Dimension !")
    D = len(initial)
    new_x=initial
    new_f=f(new_x)
    for _ in range(iterations):
        can_x=[new_x[i]+random.uniform(-radius[i],radius[i]) for i in range(D)]
       #print(can_x)
        can_f=f(can_x)
        if(can_f<new_f):
            (new_f,new_x) = (can_f,can_x)
            #print(new_f)
            #print(new_x)
    return({"minimum point":new_x,"function value":new_f})
      
initial = [0.5,0.5]
radius = [2,2]
iterations = 5000000
result=optimization(initial,radius,iterations,f)
print(result)

"""Problem 2:"""

import numpy as np
from scipy import optimize
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter


def f(z):
    x,y=z
    h=np.zeros(x.shape)
    h[x<2] = np.power(x[x<2],2)+np.power(y[x<2],2)
    h[x>2]= np.power(x[x>2]-4,2)+np.power(y[x>2],2)+0.3
    return h
  
fig = plt.figure()
ax = fig.gca(projection='3d')

X = np.arange(-2, 6, 0.002)
Y = np.arange(-2, 2, 0.002)
X, Y = np.meshgrid(X, Y)
Z=f((X,Y))

surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,
                       linewidth=0, antialiased= True) 


rranges = (slice(-4, 4,0.01), slice(-4, 4,0.01))

resbrute = optimize.brute(f, rranges,full_output=True,
                          finish=optimize.fmin)



print("The global minimum point is located at",resbrute[0])  

print("The function value at this global point is",resbrute[1])



"""# Simulated Annealing"""

#codes for SA is coming from Brigham Young University's ME 575's Optimization course, taught by Professor: John D. Hedengren. Codes are partially edited by me. - Behnam Kia
# Import some other libraries that we'll need
# matplotlib and numpy packages must also be installed
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
import random
import math

# define objective function
def f(x):
    x1 = x[0]
    x2 = x[1]
    obj = 0.2 + x1**2 + x2**2 - 0.1*math.cos(6.0*3.1415*x1) - 0.1*math.cos(6.0*3.1415*x2)
    return obj

# Start location
x_start = [0.8, -0.5]

# Design variables at mesh points
i1 = np.arange(-1.0, 1.0, 0.01)
i2 = np.arange(-1.0, 1.0, 0.01)
x1m, x2m = np.meshgrid(i1, i2)
fm = np.zeros(x1m.shape)
for i in range(x1m.shape[0]):
    for j in range(x1m.shape[1]):
        fm[i][j] = 0.2 + x1m[i][j]**2 + x2m[i][j]**2 \
             - 0.1*math.cos(6.0*3.1415*x1m[i][j]) \
             - 0.1*math.cos(6.0*3.1415*x2m[i][j])

# Create a contour plot
plt.figure()
# Specify contour lines
#lines = range(2,52,2)
# Plot contours
CS = plt.contour(x1m, x2m, fm)#,lines)
# Label contours
plt.clabel(CS, inline=1, fontsize=10)
# Add some text to the plot
plt.title('Non-Convex Function')
plt.xlabel('x1')
plt.ylabel('x2')

##################################################
# Simulated Annealing
##################################################
# Number of cycles
n = 50
# Number of trials per cycle
m = 50
# Number of accepted solutions
na = 0.0
# Probability of accepting worse solution at the start
p1 = 0.7
# Probability of accepting worse solution at the end
p50 = 0.001
# Initial temperature
t1 = -1.0/math.log(p1)
# Final temperature
t50 = -1.0/math.log(p50)
# Fractional reduction every cycle
frac = (t50/t1)**(1.0/(n-1.0))
# Initialize x
x = np.zeros((n+1,2))
x[0] = x_start
xi = np.zeros(2)
xi = x_start
na = na + 1.0
# Current best results so far
xc = np.zeros(2)
xc = x[0]
fc = f(xi)
fs = np.zeros(n+1)
fs[0] = fc
# Current temperature
t = t1
# DeltaE Average
DeltaE_avg = 0.0
for i in range(n):
    for j in range(m):
        # Generate new trial points
        xi[0] = xc[0] + random.random() - 0.5
        xi[1] = xc[1] + random.random() - 0.5
        # Clip to upper and lower bounds
        xi[0] = max(min(xi[0],1.0),-1.0)
        xi[1] = max(min(xi[1],1.0),-1.0)
        DeltaE = abs(f(xi)-fc)
        if (f(xi)>fc):
            # Initialize DeltaE_avg if a worse solution was found
            #   on the first iteration
            if (i==0 and j==0): DeltaE_avg = DeltaE
            # objective function is worse
            # generate probability of acceptance
            p = math.exp(-DeltaE/(DeltaE_avg * t))
            # determine whether to accept worse point
            if (random.random()<p):
                # accept the worse solution
                accept = True
            else:
                # don't accept the worse solution
                accept = False
        else:
            # objective function is lower, automatically accept
            accept = True
        if (accept==True):
            # update currently accepted solution
            xc[0] = xi[0]
            xc[1] = xi[1]
            fc = f(xc)
            # increment number of accepted solutions
            na = na + 1.0
            # update DeltaE_avg
            DeltaE_avg = (DeltaE_avg * (na-1.0) +  DeltaE) / na
    # Record the best x values at the end of every cycle
    x[i+1][0] = xc[0]
    x[i+1][1] = xc[1]
    fs[i+1] = fc
    # Lower the temperature for next cycle
    t = frac * t

# print solution
print('Best solution: ' + str(xc))
print('Best objective: ' + str(fc))

plt.plot(x[:,0],x[:,1],'y-o')
plt.savefig('contour.png')

fig = plt.figure()
ax1 = fig.add_subplot(211)
ax1.plot(fs,'r.-')
ax1.legend(['Objective'])
ax2 = fig.add_subplot(212)
ax2.plot(x[:,0],'b.-')
ax2.plot(x[:,1],'g--')
ax2.legend(['x1','x2'])

# Save the figure as a PNG
plt.savefig('iterations.png')

plt.show()

"""# `Homework
Try SA algorithm implemented above on Rastrigin Function. It is very possible that it may not find the global minimum point at the begining. Try changing the parameters of the algorithm to find the global minimum located at 0 and 0.
"""

import math
def f(z):
    x=z[0]
    y=z[1]
    h = (x**2 - 10 * math.cos(2 * 3.14 * x)) +(y**2 - 10 * math.cos(2 * 3.14 * y)) + 20
    return (h)
  

X = np.arange(-1,1,0.01)#len 600
Y = np.arange(-1,1,0.01)# len 600

Z_x,Z_y = np.meshgrid(X,Y)# list, len is 2, each is np.arrary, with size 600 times 600
f_value = np.zeros(np.shape(Z_x))
for i in range(len(X)):
    for j in range(len(Y)):
      f_value[i][j]=f([Z_x[i][j],Z_y[i][j]])


# plot contour picture
plt.figure()
CS=plt.contour(Z_x,Z_y,f_value)
plt.clabel(CS, inline=1, fontsize=10)
# Add some text to the plot
plt.title('Non-Convex Function')
plt.xlabel('x1')
plt.ylabel('x2')

# number of circles
n=500
# number of trials
m=1000
# start probability
ps=0.7
# final probability
pf= 0.0001
# start temp
ts = -1/math.log(ps)
# final temp
tf = -1/math.log(pf)
# frac reduction around each circle
frac = (tf/ts)**(1/(n-1))


start_location = [0.5,0.5]

x = np.zeros((n+1,2))# each row for each circle
f_x = np.zeros(n+1)

x[0]=start_location
xi=np.zeros(2)

xc = x[0]
fc = f(xc)

t = ts
Delta_avg_E=0
na=0

for i in range(n):
  for j in range(m):
      # generate new points for try
      xi[0]=xc[0]+ random.random()-0.5
      xi[1]=xc[1]+ random.random()-0.5
      xi[0]=max(min(xi[0],3),-3)
      xi[1]=max(min(xi[1],3),-3)
      Delta_E=abs(f(xi)-fc)
      if(f(xi)>fc):
          if(i == 0 and j == 0): 
              Delta_avg_E=Delta_E
          p = math.exp(-Delta_E/(Delta_avg_E*t))
          if(random.random()<p):
              accept = True
          else:
              accept = False
      else:
         accept = True
      if accept == True:
          xc[0] = xi[0]
          xc[1] = xi[1]
          fc = f(xi)
          na = na+1
          Delta_avg_E = (Delta_avg_E*(na-1)+Delta_E)/na
   #record results of one circle
  x[i+1][0]=xc[0]
  x[i+1][1]=xc[1]
  f_x[i+1]=fc
  t = t*frac
  
print(fc)
print(xc)


plt.plot(x[:,0],x[:,1],'y-o')
plt.savefig('contour.png')

fig = plt.figure()
ax1 = fig.add_subplot(211)
ax1.plot(f_x,'r.-')
ax1.legend(['Objective'])
ax2 = fig.add_subplot(212)
ax2.plot(x[:,0],'b.-')
ax2.plot(x[:,1],'g--')
ax2.legend(['x1','x2'])

# Save the figure as a PNG
plt.savefig('iterations.png')

plt.show()

"""# Evolutionary Computation

#Deterministic and Stochastic Gradient Based Methods
There will be more than an entire session dedicated to training neural networks using stochastic gradient disent methods. Therefore in this session we skip studying these methods.

There are other search methods such as Hill Climbing, Tabu search, etc. Check them out yourself. Some of them are local, and some global.
"""