# -*- coding: utf-8 -*-
"""HW3: Logistic Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q5vOSKfdckNEyZaXOo2tof-1QMCw6f25

**Linear Classifier**
"""

from sklearn.datasets.samples_generator import make_blobs
from matplotlib import pyplot
from pandas import DataFrame
Data_set_size=200
X, Y = make_blobs(n_samples=300, centers=2, n_features=2,cluster_std=1.5, center_box=(-4.0, 4.0),random_state=42)
df = DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))
colors = {0:'red', 1:'blue'}
fig, ax = pyplot.subplots()
grouped = df.groupby('label')
for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])
pyplot.show()

import numpy as np
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt   

# define goal function
def ErrorForLinearReg(w0,w1,w2,x_data,y_data):
    linear_part=w0*x_data[:,0]+w1*x_data[:,1]+w2*x_data[:,2]
    y_hat=1/(np.exp(-linear_part)+1)
    return (-np.mean(y_data*np.log(y_hat)+(1-y_data)*np.log(1-y_hat)))

 

# get test data and train data
test_fraction=0.4
x_train, x_test, y_train, y_test=train_test_split (X, Y, test_size=test_fraction, random_state=6)

# prepocessing train data
training_size=X.shape[0]*(1-test_fraction)
scaler = preprocessing.StandardScaler().fit(x_train)
x_train_scaled=scaler.transform(x_train)
x_train_scaledp=np.c_[np.ones((int(training_size))),x_train_scaled]  #padding x_train with ones
x_test_scaledp=scaler.transform(x_test)

"""**Batch Gradient Descent**"""

import math

def BGD(w0,w1,w2,x_data,y_data):
  linear_part=w0*x_data[:,0]+w1*x_data[:,1]+w2*x_data[:,2]
  y_hat=1/(np.exp(-linear_part)+1)
  a=y_hat-y_data
  return(a.dot(x_data)/x_data.shape[0])


alpha=0.03 # learning rate fixed
max_iterations=100
max_precision=math.pow(10,-10)
w=np.random.rand(3)
iteration=0
precision=2

Wtrajectory=w
Jtrajectory=np.array(ErrorForLinearReg(w[0],w[1],w[2],x_train_scaledp,y_train))

while iteration<max_iterations and precision>max_precision:
  iteration +=1
  #print(iteration)
  gradient=BGD(w[0],w[1],w[2],x_train_scaledp,y_train)
  w = w-alpha*gradient
  f = ErrorForLinearReg(w[0],w[1],w[2],x_train_scaledp,y_train)
  #precision = np.sqrt(np.sum(np.power(gradient,2)))*alpha
  #print("w",w)
  Wtrajectory=np.vstack([Wtrajectory,w])
  Jtrajectory=np.hstack([Jtrajectory,f])
  precision = abs(Jtrajectory[-2]-Jtrajectory[-1])
  #print("precision",precision)

#plot value of J
plt.plot(range(0,len(Jtrajectory)),Jtrajectory)
plt.title('Loss Function')
plt.xlabel('Iterations')
plt.show()


# get the true covariates for the original data
w_original=np.zeros(Wtrajectory.shape)
w_original[:,0]=Wtrajectory[:,0]-Wtrajectory[:,1]*scaler.mean_[0]/scaler.scale_[0]-Wtrajectory[:,2]*scaler.mean_[1]/scaler.scale_[1]
w_original[:,1]=Wtrajectory[:,1]/scaler.scale_[0]
w_original[:,2]=Wtrajectory[:,2]/scaler.scale_[1]

#plot regression
from sklearn.datasets.samples_generator import make_blobs
from matplotlib import pyplot
from pandas import DataFrame
Data_set_size=200
X, Y = make_blobs(n_samples=300, centers=2, n_features=2,cluster_std=1.5, center_box=(-4.0, 4.0),random_state=42)
df = DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))
colors = {0:'red', 1:'blue'}
fig, ax = pyplot.subplots()
grouped = df.groupby('label')
for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])

plt.ylim([-6,10])
    
n_epochs = 40
colors = plt.cm.jet(np.linspace(0,1,n_epochs))



temp_x = np.linspace(-6, 6, 100)
for k in range(0,11):
  i=10*k
  plt.plot(temp_x,(-temp_x*w_original[i,1]-w_original[i,0])/w_original[i,2],label="Iteration%d"%i,color=colors[k*3])
  #print(w_original[i,]/w_original[i,2])

leg = plt.legend(loc='best', ncol=5, mode="expand", shadow=True, fancybox=True)
leg.get_frame().set_alpha(0.5)
  
pyplot.show()

#

w_final=w_original[-1,]
(w0,w1,w2)=w_final
linear_part=w0+w1*x_test_scaledp[:,0]+w2*x_test_scaledp[:,1]
y_test_hat=1/(np.exp(-linear_part)+1)
y_test_hat_class=np.zeros(y_test_hat.shape[0])
y_test_hat_class[y_test_hat>0.5]=1
print("Batch Gradient Descent's error rate is %f"%np.mean(abs(y_test_hat_class-y_test)))

"""**Stochastic Gradient Descent**"""

import random
t0=7
t1=50
def learning_rate(t):
  return(t0/(t+t1))

def BGD(w0,w1,w2,x_data,y_data):
  n=random.randint(1,x_data.shape[0])-1
  linear_part=w0*x_data[n,0]+w1*x_data[n,1]+w2*x_data[n,2]
  y_hat=1/(np.exp(-linear_part)+1)
  a=y_hat-y_data[n]
  return(a*x_data[n,])


max_iterations=100
max_precision=math.pow(10,-10)
w=np.random.rand(3)
iteration=0
precision=2

Wtrajectory=w
Jtrajectory=np.array(ErrorForLinearReg(w[0],w[1],w[2],x_train_scaledp,y_train))


alpha=0.03 # learning rate fixed

while iteration<max_iterations and precision>max_precision:
  iteration +=1
  alpha = learning_rate(alpha)
  gradient=BGD(w[0],w[1],w[2],x_train_scaledp,y_train)
  w = w-alpha*gradient
  f = ErrorForLinearReg(w[0],w[1],w[2],x_train_scaledp,y_train)
  #precision = np.sqrt(np.sum(np.power(gradient,2)))*alpha
  #print("w",w)
  Wtrajectory=np.vstack([Wtrajectory,w])
  Jtrajectory=np.hstack([Jtrajectory,f])
  precision = abs(Jtrajectory[-2]-Jtrajectory[-1])
  #print("precision",precision)

#plot value of J
plt.plot(range(0,len(Jtrajectory)),Jtrajectory)
plt.title('Loss Function')
plt.xlabel('Iterations')
plt.show()


# get the true covariates for the original data
w_original=np.zeros(Wtrajectory.shape)
w_original[:,0]=Wtrajectory[:,0]-Wtrajectory[:,1]*scaler.mean_[0]/scaler.scale_[0]-Wtrajectory[:,2]*scaler.mean_[1]/scaler.scale_[1]
w_original[:,1]=Wtrajectory[:,1]/scaler.scale_[0]
w_original[:,2]=Wtrajectory[:,2]/scaler.scale_[1]

#plot regression
from sklearn.datasets.samples_generator import make_blobs
from matplotlib import pyplot
from pandas import DataFrame
Data_set_size=200
X, Y = make_blobs(n_samples=300, centers=2, n_features=2,cluster_std=1.5, center_box=(-4.0, 4.0),random_state=42)
df = DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))
colors = {0:'red', 1:'blue'}
fig, ax = pyplot.subplots()
grouped = df.groupby('label')
for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key],)
plt.ylim((-6,10))
    
n_epochs = 40
colors = plt.cm.jet(np.linspace(0,1,n_epochs))



temp_x = np.linspace(-6, 6, 100)
for k in range(0,11):
  i=10*k
  plt.plot(temp_x,(-temp_x*w_original[i,1]-w_original[i,0])/w_original[i,2],label="Iteration%d"%i,color=colors[k*3])
  #print(w_original[i,]/w_original[i,2])

leg = plt.legend(loc='best', ncol=5, mode="expand", shadow=True, fancybox=True)
leg.get_frame().set_alpha(0.5)
  
pyplot.show()

w_final=w_original[-1,]
(w0,w1,w2)=w_final
linear_part=w0+w1*x_test_scaledp[:,0]+w2*x_test_scaledp[:,1]
y_test_hat=1/(np.exp(-linear_part)+1)
y_test_hat_class=np.zeros(y_test_hat.shape[0])
y_test_hat_class[y_test_hat>0.5]=1
print("Stochastic Gradient Descent's error rate is %f"%np.mean(abs(y_test_hat_class-y_test)))

"""**Mini-Batch Gradient Descent**"""

mini_batch=40
def MBGD(w0,w1,w2,x_data,y_data):
  sample=np.random.permutation(mini_batch)
  linear_part=w0*x_data[sample,0]+w1*x_data[sample,1]+w2*x_data[sample,2]
  y_hat=1/(np.exp(-linear_part)+1)
  a=y_hat-y_data[sample]
  return(a.dot(x_data[sample,])/mini_batch)


max_iterations=100
max_precision=math.pow(10,-10)
w=np.random.rand(3)
iteration=0
precision=2

Wtrajectory=w
Jtrajectory=np.array(ErrorForLinearReg(w[0],w[1],w[2],x_train_scaledp,y_train))


alpha=0.03 # learning rate fixed

while iteration<max_iterations and precision>max_precision:
  iteration +=1
  alpha = learning_rate(alpha)
  gradient=MBGD(w[0],w[1],w[2],x_train_scaledp,y_train)
  w = w-alpha*gradient
  f = ErrorForLinearReg(w[0],w[1],w[2],x_train_scaledp,y_train)
  #precision = np.sqrt(np.sum(np.power(gradient,2)))*alpha
  #print("w",w)
  Wtrajectory=np.vstack([Wtrajectory,w])
  Jtrajectory=np.hstack([Jtrajectory,f])
  precision = abs(Jtrajectory[-2]-Jtrajectory[-1])
  #print("precision",precision)

#plot value of J
plt.plot(range(0,len(Jtrajectory)),Jtrajectory)
plt.title('Loss Function')
plt.xlabel('Iterations')
plt.show()


# get the true covariates for the original data
w_original=np.zeros(Wtrajectory.shape)
w_original[:,0]=Wtrajectory[:,0]-Wtrajectory[:,1]*scaler.mean_[0]/scaler.scale_[0]-Wtrajectory[:,2]*scaler.mean_[1]/scaler.scale_[1]
w_original[:,1]=Wtrajectory[:,1]/scaler.scale_[0]
w_original[:,2]=Wtrajectory[:,2]/scaler.scale_[1]

#plot regression
from sklearn.datasets.samples_generator import make_blobs
from matplotlib import pyplot
from pandas import DataFrame
Data_set_size=200
X, Y = make_blobs(n_samples=300, centers=2, n_features=2,cluster_std=1.5, center_box=(-4.0, 4.0),random_state=42)
df = DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))
colors = {0:'red', 1:'blue'}
fig, ax = pyplot.subplots()
grouped = df.groupby('label')
for key, group in grouped:
    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key],)
plt.ylim((-6,10))
    
n_epochs = 40
colors = plt.cm.jet(np.linspace(0,1,n_epochs))



temp_x = np.linspace(-6, 6, 100)
for k in range(0,11):
  i=10*k
  plt.plot(temp_x,(-temp_x*w_original[i,1]-w_original[i,0])/w_original[i,2],label="Iteration%d"%i,color=colors[k*3])
  #print(w_original[i,]/w_original[i,2])

leg = plt.legend(loc='best', ncol=5, mode="expand", shadow=True, fancybox=True)
leg.get_frame().set_alpha(0.5)
  
pyplot.show()

w_final=w_original[-1,]
(w0,w1,w2)=w_final
linear_part=w0+w1*x_test_scaledp[:,0]+w2*x_test_scaledp[:,1]
y_test_hat=1/(np.exp(-linear_part)+1)
y_test_hat_class=np.zeros(y_test_hat.shape[0])
y_test_hat_class[y_test_hat>0.5]=1
print("Mini-Batch Gradient Descent's error rate is %f"%np.mean(abs(y_test_hat_class-y_test)))