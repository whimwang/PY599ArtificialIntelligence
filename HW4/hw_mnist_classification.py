# -*- coding: utf-8 -*-
"""HW: MNIST Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17ATLaqyfCSsSInHr-5nkLByl1sHCA9FX

## Importing Modules
"""

from keras.datasets import mnist
import matplotlib.pyplot as plt
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils

seed = 7
numpy.random.seed(seed)
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
plt.imshow(X_train[20000], cmap=plt.get_cmap('gray'))
plt.show()
print(Y_train[20000])

import numpy as np
num_pixels=X_train.shape[1]*X_train.shape[2]
X_train_p=X_train.reshape(X_train.shape[0],num_pixels).astype('float32')
X_test_p=X_test.reshape(X_test.shape[0],num_pixels).astype('float32')


from sklearn import preprocessing
scaler=preprocessing.StandardScaler().fit(X_train_p)
X_train_p_scaled=scaler.transform(X_train_p) 
X_test_p_scaled=scaler.transform(X_test_p)

from sklearn.decomposition import PCA
new_PCA=PCA(.95) 
new_PCA.fit(X_train_p_scaled)
X_train_pca=new_PCA.transform(X_train_p_scaled)
new_PCA.n_components_

X_test_pca=new_PCA.transform(X_test_p_scaled)

#0.95 331
#0.90 236
#0.80 149
#0.70 98

Y_train = np_utils.to_categorical(Y_train)
Y_test = np_utils.to_categorical(Y_test)
num_classes = Y_test.shape[1]
Y_train.shape

X_train_add_1=np.concatenate((np.zeros((X_train_pca.shape[0],1)),X_train_pca),axis=1)
X_test_add_1=np.concatenate((np.zeros((X_test_pca.shape[0],1)),X_test_pca),axis=1)

X_train_add_1.shape
X_test_add_1.shape

my_model = Sequential()
my_model.add(Dense(20, input_dim=X_train_add_1.shape[1], activation='relu'))#121
my_model.add(Dense(num_classes, activation='softmax'))
my_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

my_model.fit(X_train_add_1, Y_train, epochs=20, batch_size=100, verbose=2)

scores = my_model.evaluate(X_test_add_1, Y_test, verbose=0)#loss and accuracy
print("Error: %.2f%%" % (100-scores[1]*100))
#0.95 8.4
#0.90 8.4
#0.80 8.6